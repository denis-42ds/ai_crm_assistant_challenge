### AI-ассистент для приоритизации лидов в CRM

---

#### **Роль пользователя и сценарий**

**Пользователь:** Менеджер по продажам.

**Проблема:** Ручная классификация входящих лидов (заявок) отнимает у менеджеров много времени. Им приходится вручную анализировать каждую заявку, чтобы понять, насколько она приоритетна. Это приводит к задержкам в обработке самых перспективных клиентов и потере потенциальных продаж.

**Сценарий:** Новый лид попадает в CRM. Наш AI-ассистент автоматически анализирует его текстовое содержимое (описание, переписку, заметки) и мгновенно присваивает ему один из трёх тегов: **«Горячий»**, **«Тёплый»** или **«Холодный»**. Менеджер получает уже приоритизированный список и сразу начинает работу с наиболее перспективными клиентами, не тратя время на низкоприоритетные запросы.

---

#### **Метрика и измеримость эффекта**

Эффект от внедрения ассистента измеряется по двум ключевым метрикам:

1.  **Сокращение времени на классификацию лида:** Ручная обработка одного лида занимает 3-5 минут. Наш AI-ассистент выполняет классификацию за несколько секунд. Это позволяет менеджеру экономить до **25%** рабочего времени, затрачиваемого на рутинные задачи.
2.  **Повышение скорости обработки «горячих» лидов:** Автоматическая приоритизация позволяет менеджеру немедленно переходить к работе с самыми перспективными клиентами, что напрямую влияет на **рост конверсии** и сокращает цикл сделки.

---

#### **Техническая реализация**

* **Стек:** Python, FastAPI, Docker.
* **Используемая LLM:** `gpt-4.1-mini` (из предоставленного списка).
* **Архитектура:** Решение реализовано в виде микросервиса на **FastAPI**. Это позволяет легко интегрировать его с любой CRM-системой, поддерживающей вебхуки и API-запросы. Код упакован в **Docker-контейнер**, что гарантирует его воспроизводимость и лёгкость развёртывания.
* **Логика:** При получении POST-запроса с текстом лида, микросервис формирует **детальный промпт** для LLM, включающий определение каждого тега. Ответ от LLM (одно слово-тег) возвращается как JSON-ответ API. Ключ API передаётся в контейнер как переменная окружения, что соответствует лучшим практикам безопасности.

---

#### **Ссылка на прототип/репозиторий**

[Вставьте сюда ссылку на ваш GitHub-репозиторий]

---

#### **Надёжность и обоснованность**

Надёжность решения достигается за счёт:

* **Чётко сформулированного промпта**, который минимизирует галлюцинации LLM. Мы добавили строгие определения для каждого тега, чтобы модель давала точные и релевантные ответы.
* **Обработки ошибок** при взаимодействии с LLM API, что делает сервис отказоустойчивым.
